---
title: Testing Overview
sidebar_position: 8
---

# Testing Overview

<<<<<<< HEAD
Archon employs a comprehensive testing strategy with **235+ test cases** covering unit tests, integration tests, end-to-end tests, and performance testing across all components of the system.
=======
Archon employs a comprehensive testing strategy across both backend (Python/pytest) and frontend (React/Vitest) components. This guide provides an overview of our testing approach, with detailed strategies available for each technology stack.
>>>>>>> origin/main

## ğŸ¯ Core Testing Principles

<<<<<<< HEAD
Our testing approach follows the testing pyramid with extensive coverage:
=======
1. **Test at the Right Level**: Follow the testing pyramid - many unit tests, fewer integration tests, minimal E2E tests
2. **Fast Feedback**: Tests should run quickly to enable rapid development
3. **Isolation**: Each test should be independent and repeatable
4. **User-Focused**: Test behavior and outcomes, not implementation details
5. **Comprehensive Coverage**: Aim for high coverage of critical paths

## ğŸ—ï¸ Testing Architecture
>>>>>>> origin/main

```mermaid
%%{init:{
  'theme':'base',
  'themeVariables': {
    'primaryColor':'#1f2937',
    'primaryTextColor':'#ffffff',
    'primaryBorderColor':'#8b5cf6',
    'lineColor':'#a855f7',
    'textColor':'#ffffff',
    'fontFamily':'Inter',
    'fontSize':'14px',
    'background':'#000000',
    'mainBkg':'#1f2937',
    'secondBkg':'#111827',
    'borderColor':'#8b5cf6'
  }
}}%%
graph TB
<<<<<<< HEAD
    subgraph "Testing Pyramid"
        E2E["E2E Tests<br/>ğŸ” User Journeys<br/>WebSocket/API Integration"]
        Integration["Integration Tests<br/>ğŸ”— API & Database<br/>FastAPI TestClient"]
        Unit["Unit Tests (235+ cases)<br/>âš¡ Services & Components<br/>pytest + AsyncMock"]
=======
    subgraph "Backend Testing (Python)"
        PyUnit["Unit Tests<br/>âš¡ pytest + mocks<br/>Services, Utils, Models"]
        PyInt["Integration Tests<br/>ğŸ”— FastAPI TestClient<br/>API Endpoints, Database"]
        PyE2E["E2E Tests<br/>ğŸ” Full Workflows<br/>MCP Tools, WebSockets"]
>>>>>>> origin/main
    end

    subgraph "Frontend Testing (React)"
        JSUnit["Component Tests<br/>âš¡ Vitest + RTL<br/>Components, Hooks"]
        JSInt["Integration Tests<br/>ğŸ”— Page Tests<br/>User Flows, API Mocks"]
        JSE2E["E2E Tests<br/>ğŸ” Playwright<br/>Full Application"]
    end

    subgraph "Test Execution"
        Local["Local Development<br/>npm test, pytest"]
        CI["CI/CD Pipeline<br/>GitHub Actions"]
        UI["UI Test Runner<br/>Settings Page"]
    end

    PyUnit --> PyInt
    PyInt --> PyE2E
    JSUnit --> JSInt
    JSInt --> JSE2E
    
    PyE2E --> Local
    JSE2E --> Local
    Local --> CI
    Local --> UI
```

## ğŸš€ UI Test Runner

Archon includes a built-in test runner accessible from the Settings page in the UI. This allows you to:

- **Run Tests On-Demand**: Execute Python and React test suites without command line access
- **Real-time Output**: See test results streamed in real-time via WebSockets
- **Test History**: View previous test runs and their results
- **Visual Feedback**: Green/red indicators for pass/fail status
- **Coverage Reports**: Access HTML coverage reports directly from the UI

### Accessing the Test Runner

1. Navigate to **Settings** in the Archon UI
2. Click on the **Tests** tab
3. Select which test suite to run:
   - **Python Tests**: Runs pytest suite for backend
   - **React Tests**: Runs Vitest suite for frontend
4. Click **Run Tests** to execute
5. View real-time output and results

## ğŸ“‹ Testing Strategy Documents

For detailed testing strategies and best practices:

- **[Python Testing Strategy](./testing-python-strategy)** - Comprehensive pytest guide for backend testing
- **[Vitest Testing Strategy](./testing-vitest-strategy)** - React & TypeScript testing with Vitest

## ğŸ“ Test Directory Structure

```
python/
â”œâ”€â”€ tests/                   # Backend test suite
â”‚   â”œâ”€â”€ unit/                # Unit tests (235+ test cases)
â”‚   â”‚   â”œâ”€â”€ test_agents/     # Agent tests (BaseAgent, DocumentAgent, RagAgent)
â”‚   â”‚   â”œâ”€â”€ test_api/        # API endpoint tests
â”‚   â”‚   â”œâ”€â”€ test_models/     # Model validation tests
â”‚   â”‚   â”œâ”€â”€ test_modules/    # Module tests (project, rag)
â”‚   â”‚   â”œâ”€â”€ test_services/   # Service layer tests
â”‚   â”‚   â”‚   â”œâ”€â”€ test_projects/   # Project service tests
â”‚   â”‚   â”‚   â””â”€â”€ test_rag/        # RAG service tests
â”‚   â”‚   â””â”€â”€ test_utils/      # Utility function tests
â”‚   â”œâ”€â”€ integration/         # Integration tests
â”‚   â”‚   â”œâ”€â”€ test_api/        # API integration tests
â”‚   â”‚   â”œâ”€â”€ test_database/   # Database integration
â”‚   â”‚   â”œâ”€â”€ test_mcp/        # MCP integration
â”‚   â”‚   â””â”€â”€ test_websockets/ # WebSocket tests
â”‚   â”œâ”€â”€ e2e/                 # End-to-end tests
â”‚   â”‚   â”œâ”€â”€ test_workflows/  # Complete user workflows
â”‚   â”‚   â””â”€â”€ test_mcp_tools/  # MCP tool integration
â”‚   â”œâ”€â”€ performance/         # Performance tests
â”‚   â”‚   â””â”€â”€ test_performance.py
â”‚   â”œâ”€â”€ fixtures/            # Test fixtures and data
â”‚   â”‚   â”œâ”€â”€ mock_data.py     # Factory patterns for test data
â”‚   â”‚   â””â”€â”€ test_helpers.py  # Testing utilities
â”‚   â”œâ”€â”€ conftest.py         # Pytest configuration
â”‚   â”œâ”€â”€ pytest.ini          # Pytest settings
â”‚   â”œâ”€â”€ .coveragerc         # Coverage configuration
â”‚   â””â”€â”€ run_tests.py        # Test runner with filtering
â”œâ”€â”€ src/                     # Application source code
â””â”€â”€ pyproject.toml          # Python project configuration
```

## ğŸš€ Quick Start

### Prerequisites

```bash
# Navigate to python directory
cd python

# Install dependencies using uv
uv sync

# Or install test dependencies manually
uv pip install pytest pytest-asyncio pytest-cov pytest-timeout pytest-mock
```

### Running Tests

```bash
# Run all tests using the test runner
python tests/run_tests.py

# Run tests by type
python tests/run_tests.py --type unit
python tests/run_tests.py --type integration
python tests/run_tests.py --type e2e

# Run tests by priority
python tests/run_tests.py --priority critical
python tests/run_tests.py --priority high
python tests/run_tests.py --priority standard

# Run with coverage report
python tests/run_tests.py --coverage

# Run specific test categories with pytest directly
PYTHONPATH=/workspace/python pytest tests/unit/test_services/ -v
PYTHONPATH=/workspace/python pytest tests/unit/test_api/ -v

# Run tests with specific markers
pytest -m "not slow"       # Skip slow tests
pytest -m "critical"       # Run critical tests only
pytest -m "websocket"      # Run WebSocket tests only
```

### Running Tests from UI

The Archon UI Settings page provides a test runner interface that connects via WebSocket to stream test results in real-time:

1. Navigate to Settings page in the UI
2. Click on "Run Tests" button
3. Select test type (Unit Tests or UI Tests)
4. View real-time test output and results

## ğŸ”§ Test Configuration

### pytest.ini

```ini
[tool:pytest]
minversion = 6.0
addopts = 
    -ra
    --strict-markers
    --strict-config
    --cov=src
    --cov-branch
    --cov-report=term-missing:skip-covered
    --cov-report=html:htmlcov
    --cov-report=xml
    --cov-fail-under=80
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
asyncio_mode = auto
asyncio_default_fixture_loop_scope = function
markers =
    slow: marks tests as slow (deselect with '-m "not slow"')
    critical: critical functionality tests
    high: high priority tests
    standard: standard priority tests
    nice_to_have: nice to have tests
    unit: unit tests
    integration: integration tests
    e2e: end-to-end tests
    performance: performance tests
    websocket: WebSocket-related tests
    mcp: MCP-related tests
    rag: RAG-related tests
    auth: authentication tests
```

### Coverage Configuration (.coveragerc)

```ini
[run]
source = src
omit = 
    */tests/*
    */test_*
    */__pycache__/*
    */venv/*
    */.venv/*
    */migrations/*
    */alembic/*
    */conftest.py
    */setup.py
    */config.py
parallel = true
concurrency = multiprocessing,thread

[report]
precision = 2
show_missing = true
skip_covered = false
fail_under = 80
exclude_lines =
    pragma: no cover
    def __repr__
    raise AssertionError
    raise NotImplementedError
    if __name__ == .__main__.:
    if TYPE_CHECKING:
    @abstractmethod
    @abc.abstractmethod
```

## ğŸ§ª Test Categories by Priority

### Critical Priority Tests (137 cases)
- **ProjectService** (15 cases): CRUD operations, validation, linked sources
- **TaskService** (14 cases): Status transitions, subtasks, archiving, WebSocket updates
- **DocumentService** (12 cases): JSONB operations, version snapshots
- **CredentialService** (13 cases): Encryption/decryption, environment variables
- **MCPClientService** (12 cases): SSE transport, tool execution, auto-reconnection
- **VersioningService** (10 cases): Version history, restoration
- **MCPSessionManager** (12 cases): Session lifecycle, expiration
- **CrawlingService** (11 cases): Single/batch crawling, robots.txt
- **DocumentStorageService** (13 cases): Chunking, embeddings, batch operations
- **SearchService** (13 cases): Vector search, reranking, hybrid search
- **Utils** (12 cases): Embeddings, Supabase client, text extraction

### High Priority Tests (20 cases)
- **PromptService** (10 cases): Singleton pattern, template caching
- **SourceManagementService** (10 cases): Source CRUD, metadata management

### Standard Priority Tests (78+ cases)
- **BaseAgent** (10 cases): Rate limiting, streaming, timeout handling
- **DocumentAgent** (8 cases): Conversational document interactions
- **RagAgent** (12 cases): Search and retrieval operations
- **API Endpoints**: Projects, Settings, Knowledge, MCP, Agent Chat
- **Configuration**: Environment validation, settings management

## ğŸ” Key Testing Patterns

### Async Testing

```python
import pytest
from unittest.mock import AsyncMock, patch

@pytest.mark.asyncio
async def test_async_operation(mock_service):
    """Test async operations with proper mocking."""
    # Arrange
    mock_service.async_method = AsyncMock(return_value={"status": "success"})
    
    # Act
    result = await mock_service.async_method()
    
    # Assert
    assert result["status"] == "success"
    mock_service.async_method.assert_called_once()
```

### Parametrized Testing

```python
@pytest.mark.parametrize("status,expected", [
    ("todo", True),
    ("in_progress", True),
    ("done", False),
    ("archived", False),
])
def test_task_status_validation(status, expected):
    """Test task status validation with multiple inputs."""
    result = is_task_active(status)
    assert result == expected
```

### WebSocket Testing

```python
async def test_websocket_connection(test_client):
    """Test WebSocket connection and message handling."""
    async with test_client.websocket_connect("/ws/test") as websocket:
        # Send message
        await websocket.send_json({"type": "ping"})
        
        # Receive response
        response = await websocket.receive_json()
        assert response["type"] == "pong"
```

### Performance Testing

```python
@pytest.mark.slow
@pytest.mark.performance
async def test_bulk_operation_performance(benchmark_service):
    """Test performance of bulk operations."""
    items = generate_test_items(1000)
    
    start_time = time.time()
    results = await benchmark_service.bulk_process(items)
    duration = time.time() - start_time
    
    assert len(results) == 1000
    assert duration < 5.0  # Should complete within 5 seconds
```

## ğŸ“Š Coverage Goals

- **Overall Coverage**: 80% minimum
- **Critical Services**: 95% minimum
- **API Endpoints**: 90% minimum
- **Utility Functions**: 85% minimum

Current coverage can be viewed by running:
```bash
python tests/run_tests.py --coverage
# HTML report available at htmlcov/index.html
```

## ğŸš¨ Common Issues and Solutions

### Python 3.13 Compatibility
If you encounter OpenAI library compatibility issues with Python 3.13:
```bash
# Use Python 3.12
uv python install 3.12
uv venv --python 3.12
uv sync
```

### Import Errors
Ensure PYTHONPATH is set correctly:
```bash
export PYTHONPATH=/workspace/python
# or
PYTHONPATH=/workspace/python pytest tests/
```

### WebSocket Test Timeouts
Increase timeout for WebSocket tests:
```python
@pytest.mark.timeout(30)  # 30 second timeout
async def test_long_websocket_operation():
    # test code
```

<<<<<<< HEAD
## ğŸ”„ Continuous Integration

Tests are automatically run on:
- Pull request creation
- Commits to main branch
- Nightly builds
- Manual trigger from UI

The CI pipeline includes:
1. Unit tests with coverage report
2. Integration tests
3. E2E tests (if applicable)
4. Performance benchmarks
5. Security scanning
=======
## ğŸš€ Quick Start Guide

### Running Backend Tests (Python)

```bash
# Navigate to Python directory
cd python

# Run all tests
pytest

# Run with coverage
pytest --cov=src --cov-report=html

# Run specific test category
pytest -m unit
pytest -m integration
pytest -m mcp

# Run in watch mode (requires pytest-watch)
ptw
```

### Running Frontend Tests (React)

```bash
# Navigate to UI directory
cd archon-ui-main

# Run all tests
npm test

# Run with coverage
npm run test:coverage

# Run with UI
npm run test:ui

# Run in watch mode
npm run test:watch
```

### Running Tests via Docker

```bash
# Run Python tests in Docker
docker-compose run --rm archon-pyserver pytest

# Run React tests in Docker
docker-compose run --rm archon-ui npm test
```

## ğŸ“ˆ Coverage Goals

| Component | Target Coverage | Critical Path Coverage |
|-----------|----------------|----------------------|
| Python Backend | 80% | 95% |
| React Frontend | 80% | 90% |
| MCP Tools | 90% | 100% |
| API Endpoints | 85% | 95% |
| Utilities | 95% | 100% |

## ğŸ”„ Continuous Integration

Our CI pipeline automatically runs tests on every push and pull request:

1. **Pre-commit Hooks**: Run linting and type checking
2. **Unit Tests**: Fast feedback on component functionality
3. **Integration Tests**: Verify component interactions
4. **Coverage Reports**: Track and enforce coverage requirements
5. **E2E Tests**: Validate critical user journeys

## ğŸ› ï¸ Testing Tools & Frameworks

### Backend (Python)
- **pytest**: Testing framework
- **pytest-asyncio**: Async test support
- **pytest-mock**: Mocking utilities
- **httpx**: Async HTTP testing
- **coverage.py**: Code coverage

### Frontend (React)
- **Vitest**: Test runner and framework
- **React Testing Library**: Component testing
- **MSW**: API mocking
- **@testing-library/user-event**: User interaction simulation
- **Playwright**: E2E testing

## ğŸ“š Best Practices

1. **Write Tests First**: Follow TDD when implementing new features
2. **Test Behavior, Not Implementation**: Focus on what the code does, not how
3. **Keep Tests Simple**: Each test should verify one thing
4. **Use Descriptive Names**: Test names should explain what is being tested
5. **Mock External Dependencies**: Keep tests fast and deterministic
6. **Regular Test Maintenance**: Update tests when requirements change

---

**Next Steps:**
- Deep dive into [Python Testing Strategy](./testing-python-strategy) for backend testing details
- Explore [Vitest Testing Strategy](./testing-vitest-strategy) for frontend testing patterns
- Learn about [Deployment](./deployment) for CI/CD setup
- Check [API Reference](./api-reference) for endpoint documentation
>>>>>>> origin/main
