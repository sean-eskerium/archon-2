---
title: Testing Overview
sidebar_position: 8
---

# Testing Overview

Archon employs a comprehensive testing strategy with **235+ test cases** across both backend (Python/pytest) and frontend (React/Vitest) components. This guide provides an overview of our testing approach, with detailed strategies available for each technology stack.

## ğŸ¯ Core Testing Principles

1. **Test at the Right Level**: Follow the testing pyramid - many unit tests, fewer integration tests, minimal E2E tests
2. **Fast Feedback**: Tests should run quickly to enable rapid development
3. **Isolation**: Each test should be independent and repeatable
4. **User-Focused**: Test behavior and outcomes, not implementation details
5. **Comprehensive Coverage**: Aim for high coverage of critical paths

## ğŸ—ï¸ Testing Architecture

```mermaid
graph TB
    subgraph "Backend Testing (Python)"
        PyUnit["Unit Tests (235+ cases)<br/>âš¡ pytest + mocks<br/>Services, Utils, Models"]
        PyInt["Integration Tests<br/>ğŸ”— FastAPI TestClient<br/>API Endpoints, Database"]
        PyE2E["E2E Tests<br/>ğŸ” Full Workflows<br/>MCP Tools, WebSockets"]
    end

    subgraph "Frontend Testing (React)"
        JSUnit["Component Tests<br/>âš¡ Vitest + RTL<br/>Components, Hooks"]
        JSInt["Integration Tests<br/>ğŸ”— Page Tests<br/>User Flows, API Mocks"]
        JSE2E["E2E Tests<br/>ğŸ” Playwright<br/>Full Application"]
    end

    subgraph "Test Execution & Reporting"
        UI["UI Test Runner<br/>Settings Page<br/>Real-time Results"]
        Local["Local Development<br/>npm test, pytest"]
        CI["CI/CD Pipeline<br/>GitHub Actions"]
        Coverage["Coverage Reports<br/>HTML, JSON, LCOV"]
    end

    PyUnit --> PyInt
    PyInt --> PyE2E
    JSUnit --> JSInt
    JSInt --> JSE2E
    
    PyE2E --> UI
    JSE2E --> UI
    UI --> Coverage
    Local --> CI
    UI --> Local
```

## ğŸš€ UI Test Runner & Results Dashboard

Archon includes a built-in test runner with comprehensive reporting accessible from the Settings page. This provides:

### **Test Results Modal Features**
- **Test Health Score**: Executive summary combining test success rate + coverage percentage
- **Test Summary**: Pass/fail counts, duration, and individual test suite breakdown
- **Coverage Analysis**: Lines, functions, statements, and branches coverage with visual progress bars
- **Color-coded Indicators**: Green â‰¥80%, Yellow â‰¥60%, Red `<60%`
- **Real-time Updates**: Button appears automatically after successful test runs

### **Accessing the Test Runner**
1. Navigate to **Settings** in the Archon UI
2. Scroll to the **Archon Unit Tests** section
3. Run React UI tests to generate test results and coverage
4. Click **Test Results** button (appears after tests complete)
5. View comprehensive test health report in modal

### **Key Capabilities**
- **On-Demand Execution**: Run Python and React test suites without command line
- **Streaming Output**: Real-time test results via WebSockets
- **Visual Feedback**: Color-coded status indicators and progress bars
- **Coverage Integration**: Automatic HTML coverage report generation
- **Test History**: Previous test run tracking

## ğŸ“‹ Testing Strategy Documents

For detailed testing strategies and best practices:

- **[Vitest Testing Strategy](./testing-vitest-strategy)** - Comprehensive React & TypeScript testing guide
- **[Python Testing Strategy](./testing-python-strategy)** - Backend testing with pytest

## ğŸ“ Test Directory Structure

### Frontend (React/TypeScript)

```
archon-ui-main/
â”œâ”€â”€ vitest.config.ts           # Vitest configuration with coverage
â”œâ”€â”€ vite.config.ts             # Dev server with test endpoints
â”œâ”€â”€ test/
â”‚   â”œâ”€â”€ setup.ts              # Enhanced test setup with mocks
â”‚   â”œâ”€â”€ README_TEST_GUIDE.md   # Comprehensive testing guide
â”‚   â”œâ”€â”€ TEST_STRUCTURE.md      # File structure documentation
â”‚   â”œâ”€â”€ components/           # Component tests (73 files)
â”‚   â”‚   â”œâ”€â”€ ui/               # UI component tests
â”‚   â”‚   â”œâ”€â”€ layouts/          # Layout component tests
â”‚   â”‚   â”œâ”€â”€ mcp/              # MCP component tests
â”‚   â”‚   â”œâ”€â”€ settings/         # Settings component tests
â”‚   â”‚   â””â”€â”€ project-tasks/    # Project task component tests
â”‚   â”œâ”€â”€ services/             # Service layer tests (12 files)
â”‚   â”œâ”€â”€ pages/                # Page component tests (5 files)
â”‚   â”œâ”€â”€ hooks/                # Custom hook tests (2 files)
â”‚   â”œâ”€â”€ contexts/             # Context provider tests (3 files)
â”‚   â”œâ”€â”€ lib/                  # Utility function tests (2 files)
â”‚   â”œâ”€â”€ integration/          # Integration tests (8 files)
â”‚   â”œâ”€â”€ e2e/                  # End-to-end tests (5 files)
â”‚   â”œâ”€â”€ performance/          # Performance tests (3 files)
â”‚   â”œâ”€â”€ fixtures/             # Test data and mocks
â”‚   â””â”€â”€ utils/                # Test utilities
â””â”€â”€ coverage/                 # Generated coverage reports
    â”œâ”€â”€ index.html            # HTML coverage report
    â”œâ”€â”€ coverage-summary.json # Coverage summary
    â””â”€â”€ test-results.json     # Test execution results
```

### Backend (Python)

```
python/
<<<<<<< HEAD
â”œâ”€â”€ tests/                   # Backend test suite
â”‚   â”œâ”€â”€ unit/                # Unit tests (235+ test cases)
â”‚   â”‚   â”œâ”€â”€ test_agents/     # Agent tests (BaseAgent, DocumentAgent, RagAgent)
â”‚   â”‚   â”œâ”€â”€ test_api/        # API endpoint tests
â”‚   â”‚   â”œâ”€â”€ test_models/     # Model validation tests
â”‚   â”‚   â”œâ”€â”€ test_modules/    # Module tests (project, rag)
â”‚   â”‚   â”œâ”€â”€ test_services/   # Service layer tests
â”‚   â”‚   â”‚   â”œâ”€â”€ test_projects/   # Project service tests
â”‚   â”‚   â”‚   â””â”€â”€ test_rag/        # RAG service tests
â”‚   â”‚   â””â”€â”€ test_utils/      # Utility function tests
â”‚   â”œâ”€â”€ integration/         # Integration tests
â”‚   â”‚   â”œâ”€â”€ test_api/        # API integration tests
â”‚   â”‚   â”œâ”€â”€ test_database/   # Database integration
â”‚   â”‚   â”œâ”€â”€ test_mcp/        # MCP integration
â”‚   â”‚   â””â”€â”€ test_websockets/ # WebSocket tests
â”‚   â”œâ”€â”€ e2e/                 # End-to-end tests
â”‚   â”‚   â”œâ”€â”€ test_workflows/  # Complete user workflows
â”‚   â”‚   â””â”€â”€ test_mcp_tools/  # MCP tool integration
â”‚   â”œâ”€â”€ performance/         # Performance tests
â”‚   â”‚   â””â”€â”€ test_performance.py
â”‚   â”œâ”€â”€ fixtures/            # Test fixtures and data
â”‚   â”‚   â”œâ”€â”€ mock_data.py     # Factory patterns for test data
â”‚   â”‚   â””â”€â”€ test_helpers.py  # Testing utilities
â”‚   â”œâ”€â”€ conftest.py         # Pytest configuration
â”‚   â”œâ”€â”€ pytest.ini          # Pytest settings
â”‚   â”œâ”€â”€ .coveragerc         # Coverage configuration
â”‚   â””â”€â”€ run_tests.py        # Test runner with filtering
=======
â”œâ”€â”€ tests/                    # Backend test suite
â”‚   â”œâ”€â”€ unit/                # Unit tests
â”‚   â”œâ”€â”€ integration/         # Integration tests
â”‚   â”œâ”€â”€ e2e/                 # End-to-end tests
â”‚   â”œâ”€â”€ performance/         # Performance tests
â”‚   â”œâ”€â”€ fixtures/            # Test data
â”‚   â”œâ”€â”€ conftest.py         # Pytest configuration
â”‚   â””â”€â”€ pytest.ini         # Pytest settings
>>>>>>> origin/dev
â”œâ”€â”€ src/                     # Application source code
â””â”€â”€ pyproject.toml          # Python project configuration
```

## ğŸ”§ Test Configuration

### Frontend Vitest Configuration

<<<<<<< HEAD
```bash
# Navigate to python directory
cd python

# Install dependencies using uv
uv sync

# Or install test dependencies manually
uv pip install pytest pytest-asyncio pytest-cov pytest-timeout pytest-mock
```

### Running Backend Tests (Python)

```bash
# Run all tests using the test runner
python tests/run_tests.py

# Run tests by type
python tests/run_tests.py --type unit
python tests/run_tests.py --type integration
python tests/run_tests.py --type e2e

# Run tests by priority
python tests/run_tests.py --priority critical
python tests/run_tests.py --priority high
python tests/run_tests.py --priority standard

# Run with coverage report
python tests/run_tests.py --coverage

# Run specific test categories with pytest directly
PYTHONPATH=/workspace/python pytest tests/unit/test_services/ -v
PYTHONPATH=/workspace/python pytest tests/unit/test_api/ -v

# Run tests with specific markers
pytest -m "not slow"       # Skip slow tests
pytest -m "critical"       # Run critical tests only
pytest -m "websocket"      # Run WebSocket tests only

# Run in watch mode (requires pytest-watch)
ptw
```

### Running Frontend Tests (React)

```bash
# Navigate to UI directory
=======
```typescript
// vitest.config.ts
export default defineConfig({
  plugins: [react()],
  test: {
    globals: true,
    environment: 'jsdom',
    setupFiles: './test/setup.ts',
    include: ['test/**/*.{test,spec}.{js,mjs,cjs,ts,mts,cts,jsx,tsx}'],
    exclude: ['node_modules', 'dist', '.git', '.cache'],
    reporters: ['default', 'json'],
    outputFile: { 
      json: './coverage/test-results.json' 
    },
    coverage: {
      provider: 'v8',
      reporter: [
        'text-summary', 
        'html', 
        'json', 
        'json-summary',
        'lcov'
      ],
      reportsDirectory: './coverage',
      thresholds: {
        global: {
          statements: 70,
          branches: 65,
          functions: 70,
          lines: 70,
        },
        'src/services/**/*.ts': {
          statements: 80,
          branches: 75,
          functions: 80,
          lines: 80,
        }
      }
    },
  },
})
```

### Enhanced Test Setup

```typescript
// test/setup.ts - Comprehensive test environment setup
import '@testing-library/jest-dom'
import { vi } from 'vitest'

// Comprehensive Lucide React icon mocks (170+ icons)
vi.mock('lucide-react', () => ({
  Settings: ({ className, ...props }: any) => 
    React.createElement('span', { 
      className, 
      'data-testid': 'settings-icon',
      'data-lucide': 'Settings',
      ...props 
    }, 'Settings'),
  // ... 170+ icon mocks
}))

// Enhanced WebSocket mocking with lifecycle simulation
export class MockWebSocket {
  static CONNECTING = 0
  static OPEN = 1
  static CLOSING = 2
  static CLOSED = 3

  constructor(url: string) {
    this.url = url
    // Simulate connection opening asynchronously
    setTimeout(() => {
      this.readyState = MockWebSocket.OPEN
      if (this.onopen) {
        this.onopen(new Event('open'))
      }
    }, 0)
  }

  send = vi.fn()
  close = vi.fn(() => {
    this.readyState = MockWebSocket.CLOSED
    if (this.onclose) {
      this.onclose(new CloseEvent('close'))
    }
  })
}

// Enhanced WebSocket service mock
vi.mock('@/services/websocketService', () => ({
  websocketService: {
    connect: vi.fn().mockResolvedValue(undefined),
    disconnect: vi.fn(),
    subscribe: vi.fn().mockReturnValue(vi.fn()),
    send: vi.fn(),
    getConnectionState: vi.fn().mockReturnValue('connected')
  }
}))
```

## ğŸš€ Quick Start Commands

### Frontend Testing (React/Vitest)

```bash
# Navigate to frontend directory
>>>>>>> origin/dev
cd archon-ui-main

# Run all tests
npm test

<<<<<<< HEAD
# Run with coverage
npm run test:coverage

# Run with UI
npm run test:ui

# Run in watch mode
npm run test:watch
```

### Running Tests via Docker

```bash
# Run Python tests in Docker
docker-compose run --rm archon-pyserver pytest

# Run React tests in Docker
docker-compose run --rm archon-ui npm test
```

### Running Tests from UI

The Archon UI Settings page provides a test runner interface that connects via WebSocket to stream test results in real-time:

1. Navigate to Settings page in the UI
2. Click on "Run Tests" button
3. Select test type (Unit Tests or UI Tests)
4. View real-time test output and results

## ğŸ”§ Test Configuration

### pytest.ini

```ini
[tool:pytest]
minversion = 6.0
addopts = 
    -ra
    --strict-markers
    --strict-config
    --cov=src
    --cov-branch
    --cov-report=term-missing:skip-covered
    --cov-report=html:htmlcov
    --cov-report=xml
    --cov-fail-under=80
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
asyncio_mode = auto
asyncio_default_fixture_loop_scope = function
markers =
    slow: marks tests as slow (deselect with '-m "not slow"')
    critical: critical functionality tests
    high: high priority tests
    standard: standard priority tests
    nice_to_have: nice to have tests
    unit: unit tests
    integration: integration tests
    e2e: end-to-end tests
    performance: performance tests
    websocket: WebSocket-related tests
    mcp: MCP-related tests
    rag: RAG-related tests
    auth: authentication tests
```

### Coverage Configuration (.coveragerc)

```ini
[run]
source = src
omit = 
    */tests/*
    */test_*
    */__pycache__/*
    */venv/*
    */.venv/*
    */migrations/*
    */alembic/*
    */conftest.py
    */setup.py
    */config.py
parallel = true
concurrency = multiprocessing,thread

[report]
precision = 2
show_missing = true
skip_covered = false
fail_under = 80
exclude_lines =
    pragma: no cover
    def __repr__
    raise AssertionError
    raise NotImplementedError
    if __name__ == .__main__.:
    if TYPE_CHECKING:
    @abstractmethod
    @abc.abstractmethod
```

## ğŸ§ª Test Categories by Priority

### Critical Priority Tests (137 cases)
- **ProjectService** (15 cases): CRUD operations, validation, linked sources
- **TaskService** (14 cases): Status transitions, subtasks, archiving, WebSocket updates
- **DocumentService** (12 cases): JSONB operations, version snapshots
- **CredentialService** (13 cases): Encryption/decryption, environment variables
- **MCPClientService** (12 cases): SSE transport, tool execution, auto-reconnection
- **VersioningService** (10 cases): Version history, restoration
- **MCPSessionManager** (12 cases): Session lifecycle, expiration
- **CrawlingService** (11 cases): Single/batch crawling, robots.txt
- **DocumentStorageService** (13 cases): Chunking, embeddings, batch operations
- **SearchService** (13 cases): Vector search, reranking, hybrid search
- **Utils** (12 cases): Embeddings, Supabase client, text extraction

### High Priority Tests (20 cases)
- **PromptService** (10 cases): Singleton pattern, template caching
- **SourceManagementService** (10 cases): Source CRUD, metadata management

### Standard Priority Tests (78+ cases)
- **BaseAgent** (10 cases): Rate limiting, streaming, timeout handling
- **DocumentAgent** (8 cases): Conversational document interactions
- **RagAgent** (12 cases): Search and retrieval operations
- **API Endpoints**: Projects, Settings, Knowledge, MCP, Agent Chat
- **Configuration**: Environment validation, settings management

## ğŸ” Key Testing Patterns

### Async Testing

```python
import pytest
from unittest.mock import AsyncMock, patch

@pytest.mark.asyncio
async def test_async_operation(mock_service):
    """Test async operations with proper mocking."""
    # Arrange
    mock_service.async_method = AsyncMock(return_value={"status": "success"})
    
    # Act
    result = await mock_service.async_method()
    
    # Assert
    assert result["status"] == "success"
    mock_service.async_method.assert_called_once()
```

### Parametrized Testing

```python
@pytest.mark.parametrize("status,expected", [
    ("todo", True),
    ("in_progress", True),
    ("done", False),
    ("archived", False),
])
def test_task_status_validation(status, expected):
    """Test task status validation with multiple inputs."""
    result = is_task_active(status)
    assert result == expected
```

### WebSocket Testing

```python
async def test_websocket_connection(test_client):
    """Test WebSocket connection and message handling."""
    async with test_client.websocket_connect("/ws/test") as websocket:
        # Send message
        await websocket.send_json({"type": "ping"})
        
        # Receive response
        response = await websocket.receive_json()
        assert response["type"] == "pong"
```

### Performance Testing

```python
@pytest.mark.slow
@pytest.mark.performance
async def test_bulk_operation_performance(benchmark_service):
    """Test performance of bulk operations."""
    items = generate_test_items(1000)
    
    start_time = time.time()
    results = await benchmark_service.bulk_process(items)
    duration = time.time() - start_time
    
    assert len(results) == 1000
    assert duration < 5.0  # Should complete within 5 seconds
```

## ğŸ“Š Coverage Goals

| Component | Target Coverage | Critical Path Coverage |
|-----------|----------------|----------------------|
| Python Backend | 80% | 95% |
| React Frontend | 80% | 90% |
| MCP Tools | 90% | 100% |
| API Endpoints | 85% | 95% |
| Utilities | 95% | 100% |

Current coverage can be viewed by running:
```bash
python tests/run_tests.py --coverage
# HTML report available at htmlcov/index.html
```

## ğŸš¨ Common Issues and Solutions

### Python 3.13 Compatibility
If you encounter OpenAI library compatibility issues with Python 3.13:
```bash
# Use Python 3.12
uv python install 3.12
uv venv --python 3.12
uv sync
```

### Import Errors
Ensure PYTHONPATH is set correctly:
```bash
export PYTHONPATH=/workspace/python
# or
PYTHONPATH=/workspace/python pytest tests/
```

### WebSocket Test Timeouts
Increase timeout for WebSocket tests:
```python
@pytest.mark.timeout(30)  # 30 second timeout
async def test_long_websocket_operation():
    # test code
```

## ğŸ”„ Continuous Integration

Our CI pipeline automatically runs tests on every push and pull request:

1. **Pre-commit Hooks**: Run linting and type checking
2. **Unit Tests**: Fast feedback on component functionality
3. **Integration Tests**: Verify component interactions
4. **Coverage Reports**: Track and enforce coverage requirements
5. **E2E Tests**: Validate critical user journeys
6. **Performance Benchmarks**: Monitor for performance regressions
7. **Security Scanning**: Check for vulnerabilities

Tests are automatically run on:
- Pull request creation
- Commits to main branch
- Nightly builds
- Manual trigger from UI

## ğŸ› ï¸ Testing Tools & Frameworks

### Backend (Python)
- **pytest**: Testing framework
- **pytest-asyncio**: Async test support
- **pytest-mock**: Mocking utilities
- **pytest-cov**: Coverage reporting
- **pytest-timeout**: Test timeout management
- **httpx**: Async HTTP testing
- **coverage.py**: Code coverage

### Frontend (React)
- **Vitest**: Fast unit test runner
- **React Testing Library**: Component testing
- **MSW**: API mocking
- **Playwright**: E2E testing
- **@vitest/ui**: Interactive test UI

## ğŸ“š Additional Resources

- [Pytest Documentation](https://docs.pytest.org/)
- [Vitest Documentation](https://vitest.dev/)
- [Testing Best Practices](./PYTEST_BEST_PRACTICES_REVIEW.md)
- [Test Style Guide](./TEST_STYLE_GUIDE.md)
=======
# Run tests with coverage and results generation
npm run test:coverage

# Run tests in watch mode
npm test -- --watch

# Run tests with UI
npm run test:ui

# Run specific test file
npm test -- Button.test.tsx

# Run tests matching pattern
npm test -- --grep "should handle"
```

### Backend Testing (Python/Pytest)

```bash
# Navigate to backend directory
cd python

# Run all tests
pytest

# Run with coverage
pytest --cov=src --cov-report=html

# Run specific test categories
pytest -m "unit"
pytest -m "integration"
pytest -m "mcp"
```

### Docker Testing

```bash
# Run frontend tests in Docker
docker exec -it archon-frontend-1 npm run test:coverage

# Run backend tests in Docker
docker exec -it archon-pyserver pytest --cov=src
```

## ğŸ“Š Coverage Goals & Current Status

| Component | Target Coverage | Critical Path Coverage | Implementation Status |
|-----------|----------------|----------------------|---------------------|
| React Components | 80% | 90% | ğŸ”´ 15% (In Progress) |
| Services | 85% | 95% | ğŸ”´ 25% (In Progress) |
| Pages | 80% | 90% | ğŸŸ¡ 75% (Good Progress) |
| Hooks | 90% | 95% | ğŸ”´ 0% (Not Started) |
| Utils | 95% | 100% | ğŸ”´ 0% (Not Started) |
| **Overall Frontend** | **80%** | **90%** | **ğŸ”´ 15%** |

### Test Implementation Progress

- **Total Test Files Planned**: 68
- **Already Implemented**: 7 files âœ…
- **To Be Implemented**: 61 files ğŸ“
- **Total Test Cases**: ~465

## ğŸ”„ Test Execution Environments

### 1. Local Development
- **Command Line**: Direct npm/pytest execution
- **IDE Integration**: VS Code test runner support
- **Watch Mode**: Automatic re-execution on file changes

### 2. UI Test Runner (Settings Page)
- **Visual Interface**: Click-to-run test execution
- **Real-time Output**: Streaming test results
- **Coverage Integration**: Automatic HTML report generation
- **Test Results Modal**: Comprehensive health dashboard

### 3. Docker Environment
- **Isolated Execution**: Consistent environment across machines
- **Production-like**: Tests run in containerized environment
- **CI/CD Ready**: Same environment as deployment

### 4. Continuous Integration
- **GitHub Actions**: Automated test execution on PRs
- **Coverage Reporting**: Automatic coverage upload
- **Quality Gates**: Failed tests block merges

## ğŸ› ï¸ WebSocket Testing Strategy

WebSocket functionality is critical to Archon's real-time features. Our testing approach:

### **Safety First**
- **Never create real WebSocket connections** in tests
- **Always mock** the websocketService module
- **Clean up subscriptions** in afterEach hooks
- **Avoid function references** in useCallback dependencies

### **Mock Implementation**
```typescript
// Enhanced WebSocket mocking
vi.mock('@/services/websocketService', () => ({
  websocketService: {
    connect: vi.fn().mockResolvedValue(undefined),
    disconnect: vi.fn(),
    subscribe: vi.fn().mockReturnValue(vi.fn()),
    send: vi.fn(),
    getConnectionState: vi.fn().mockReturnValue('connected')
  }
}))
```

### **Testing Patterns**
- Test connection establishment
- Test message handling
- Test reconnection logic
- Test error scenarios
- Test cleanup and disconnection

## ğŸ¯ Quality Assurance Checklist

### Before Committing Code

- [ ] All tests pass locally
- [ ] Coverage meets minimum thresholds
- [ ] No console errors in test output
- [ ] WebSocket mocks are properly implemented
- [ ] Test names are descriptive and clear

### For New Features

- [ ] Unit tests for core functionality
- [ ] Integration tests for user workflows
- [ ] Error scenario testing
- [ ] Accessibility testing
- [ ] Performance impact assessment

### For Bug Fixes

- [ ] Regression test added
- [ ] Root cause identified and tested
- [ ] Edge cases covered
- [ ] Documentation updated if needed

## ğŸ“š Best Practices Summary

1. **User-Centric Testing**: Test from the user's perspective, not implementation details
2. **Test Isolation**: Each test should be independent and repeatable
3. **Mock External Dependencies**: Keep tests fast and deterministic
4. **Descriptive Test Names**: Clearly describe what is being tested
5. **WebSocket Safety**: Always mock WebSocket connections in tests
6. **Coverage Quality**: Focus on meaningful coverage, not just numbers
7. **Regular Maintenance**: Update tests when requirements change
8. **Documentation**: Keep test documentation current and accurate

## ğŸ”— Related Documentation

- **[Vitest Testing Strategy](./testing-vitest-strategy)** - Detailed frontend testing patterns
- **[Python Testing Strategy](./testing-python-strategy)** - Backend testing guide
- **[UI Documentation](./ui)** - Component documentation and guidelines
- **[WebSocket Documentation](./websockets)** - Real-time communication patterns
- **[API Reference](./api-reference)** - Backend API testing endpoints

---

**Quick Links:**
- ğŸš€ [Get Started with Testing](./testing-vitest-strategy#quick-start)
- ğŸ“Š [View Test Coverage Reports](#coverage-goals--current-status)
- ğŸ¯ [UI Test Runner Guide](#ui-test-runner--results-dashboard)
- ğŸ”§ [Docker Testing Setup](#docker-testing)
>>>>>>> origin/dev
