---
title: Testing
sidebar_position: 8
---

# Testing Guide

Archon employs a comprehensive testing strategy with **235+ test cases** covering unit tests, integration tests, end-to-end tests, and performance testing across all components of the system.

## 🧪 Testing Philosophy

Our testing approach follows the testing pyramid with extensive coverage:

```mermaid
%%{init:{
  'theme':'base',
  'themeVariables': {
    'primaryColor':'#1f2937',
    'primaryTextColor':'#ffffff',
    'primaryBorderColor':'#8b5cf6',
    'lineColor':'#a855f7',
    'textColor':'#ffffff',
    'fontFamily':'Inter',
    'fontSize':'14px',
    'background':'#000000',
    'mainBkg':'#1f2937',
    'secondBkg':'#111827',
    'borderColor':'#8b5cf6'
  }
}}%%
graph TB
    subgraph "Testing Pyramid"
        E2E["E2E Tests<br/>🔍 User Journeys<br/>WebSocket/API Integration"]
        Integration["Integration Tests<br/>🔗 API & Database<br/>FastAPI TestClient"]
        Unit["Unit Tests (235+ cases)<br/>⚡ Services & Components<br/>pytest + AsyncMock"]
    end

    Unit --> Integration
    Integration --> E2E
```

## 📁 Test Directory Structure

```
python/
├── tests/                   # Backend test suite
│   ├── unit/                # Unit tests (235+ test cases)
│   │   ├── test_agents/     # Agent tests (BaseAgent, DocumentAgent, RagAgent)
│   │   ├── test_api/        # API endpoint tests
│   │   ├── test_models/     # Model validation tests
│   │   ├── test_modules/    # Module tests (project, rag)
│   │   ├── test_services/   # Service layer tests
│   │   │   ├── test_projects/   # Project service tests
│   │   │   └── test_rag/        # RAG service tests
│   │   └── test_utils/      # Utility function tests
│   ├── integration/         # Integration tests
│   │   ├── test_api/        # API integration tests
│   │   ├── test_database/   # Database integration
│   │   ├── test_mcp/        # MCP integration
│   │   └── test_websockets/ # WebSocket tests
│   ├── e2e/                 # End-to-end tests
│   │   ├── test_workflows/  # Complete user workflows
│   │   └── test_mcp_tools/  # MCP tool integration
│   ├── performance/         # Performance tests
│   │   └── test_performance.py
│   ├── fixtures/            # Test fixtures and data
│   │   ├── mock_data.py     # Factory patterns for test data
│   │   └── test_helpers.py  # Testing utilities
│   ├── conftest.py         # Pytest configuration
│   ├── pytest.ini          # Pytest settings
│   ├── .coveragerc         # Coverage configuration
│   └── run_tests.py        # Test runner with filtering
├── src/                     # Application source code
└── pyproject.toml          # Python project configuration
```

## 🚀 Quick Start

### Prerequisites

```bash
# Navigate to python directory
cd python

# Install dependencies using uv
uv sync

# Or install test dependencies manually
uv pip install pytest pytest-asyncio pytest-cov pytest-timeout pytest-mock
```

### Running Tests

```bash
# Run all tests using the test runner
python tests/run_tests.py

# Run tests by type
python tests/run_tests.py --type unit
python tests/run_tests.py --type integration
python tests/run_tests.py --type e2e

# Run tests by priority
python tests/run_tests.py --priority critical
python tests/run_tests.py --priority high
python tests/run_tests.py --priority standard

# Run with coverage report
python tests/run_tests.py --coverage

# Run specific test categories with pytest directly
PYTHONPATH=/workspace/python pytest tests/unit/test_services/ -v
PYTHONPATH=/workspace/python pytest tests/unit/test_api/ -v

# Run tests with specific markers
pytest -m "not slow"       # Skip slow tests
pytest -m "critical"       # Run critical tests only
pytest -m "websocket"      # Run WebSocket tests only
```

### Running Tests from UI

The Archon UI Settings page provides a test runner interface that connects via WebSocket to stream test results in real-time:

1. Navigate to Settings page in the UI
2. Click on "Run Tests" button
3. Select test type (Unit Tests or UI Tests)
4. View real-time test output and results

## 🔧 Test Configuration

### pytest.ini

```ini
[tool:pytest]
minversion = 6.0
addopts = 
    -ra
    --strict-markers
    --strict-config
    --cov=src
    --cov-branch
    --cov-report=term-missing:skip-covered
    --cov-report=html:htmlcov
    --cov-report=xml
    --cov-fail-under=80
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
asyncio_mode = auto
asyncio_default_fixture_loop_scope = function
markers =
    slow: marks tests as slow (deselect with '-m "not slow"')
    critical: critical functionality tests
    high: high priority tests
    standard: standard priority tests
    nice_to_have: nice to have tests
    unit: unit tests
    integration: integration tests
    e2e: end-to-end tests
    performance: performance tests
    websocket: WebSocket-related tests
    mcp: MCP-related tests
    rag: RAG-related tests
    auth: authentication tests
```

### Coverage Configuration (.coveragerc)

```ini
[run]
source = src
omit = 
    */tests/*
    */test_*
    */__pycache__/*
    */venv/*
    */.venv/*
    */migrations/*
    */alembic/*
    */conftest.py
    */setup.py
    */config.py
parallel = true
concurrency = multiprocessing,thread

[report]
precision = 2
show_missing = true
skip_covered = false
fail_under = 80
exclude_lines =
    pragma: no cover
    def __repr__
    raise AssertionError
    raise NotImplementedError
    if __name__ == .__main__.:
    if TYPE_CHECKING:
    @abstractmethod
    @abc.abstractmethod
```

## 🧪 Test Categories by Priority

### Critical Priority Tests (137 cases)
- **ProjectService** (15 cases): CRUD operations, validation, linked sources
- **TaskService** (14 cases): Status transitions, subtasks, archiving, WebSocket updates
- **DocumentService** (12 cases): JSONB operations, version snapshots
- **CredentialService** (13 cases): Encryption/decryption, environment variables
- **MCPClientService** (12 cases): SSE transport, tool execution, auto-reconnection
- **VersioningService** (10 cases): Version history, restoration
- **MCPSessionManager** (12 cases): Session lifecycle, expiration
- **CrawlingService** (11 cases): Single/batch crawling, robots.txt
- **DocumentStorageService** (13 cases): Chunking, embeddings, batch operations
- **SearchService** (13 cases): Vector search, reranking, hybrid search
- **Utils** (12 cases): Embeddings, Supabase client, text extraction

### High Priority Tests (20 cases)
- **PromptService** (10 cases): Singleton pattern, template caching
- **SourceManagementService** (10 cases): Source CRUD, metadata management

### Standard Priority Tests (78+ cases)
- **BaseAgent** (10 cases): Rate limiting, streaming, timeout handling
- **DocumentAgent** (8 cases): Conversational document interactions
- **RagAgent** (12 cases): Search and retrieval operations
- **API Endpoints**: Projects, Settings, Knowledge, MCP, Agent Chat
- **Configuration**: Environment validation, settings management

## 🔍 Key Testing Patterns

### Async Testing

```python
import pytest
from unittest.mock import AsyncMock, patch

@pytest.mark.asyncio
async def test_async_operation(mock_service):
    """Test async operations with proper mocking."""
    # Arrange
    mock_service.async_method = AsyncMock(return_value={"status": "success"})
    
    # Act
    result = await mock_service.async_method()
    
    # Assert
    assert result["status"] == "success"
    mock_service.async_method.assert_called_once()
```

### Parametrized Testing

```python
@pytest.mark.parametrize("status,expected", [
    ("todo", True),
    ("in_progress", True),
    ("done", False),
    ("archived", False),
])
def test_task_status_validation(status, expected):
    """Test task status validation with multiple inputs."""
    result = is_task_active(status)
    assert result == expected
```

### WebSocket Testing

```python
async def test_websocket_connection(test_client):
    """Test WebSocket connection and message handling."""
    async with test_client.websocket_connect("/ws/test") as websocket:
        # Send message
        await websocket.send_json({"type": "ping"})
        
        # Receive response
        response = await websocket.receive_json()
        assert response["type"] == "pong"
```

### Performance Testing

```python
@pytest.mark.slow
@pytest.mark.performance
async def test_bulk_operation_performance(benchmark_service):
    """Test performance of bulk operations."""
    items = generate_test_items(1000)
    
    start_time = time.time()
    results = await benchmark_service.bulk_process(items)
    duration = time.time() - start_time
    
    assert len(results) == 1000
    assert duration < 5.0  # Should complete within 5 seconds
```

## 📊 Coverage Goals

- **Overall Coverage**: 80% minimum
- **Critical Services**: 95% minimum
- **API Endpoints**: 90% minimum
- **Utility Functions**: 85% minimum

Current coverage can be viewed by running:
```bash
python tests/run_tests.py --coverage
# HTML report available at htmlcov/index.html
```

## 🚨 Common Issues and Solutions

### Python 3.13 Compatibility
If you encounter OpenAI library compatibility issues with Python 3.13:
```bash
# Use Python 3.12
uv python install 3.12
uv venv --python 3.12
uv sync
```

### Import Errors
Ensure PYTHONPATH is set correctly:
```bash
export PYTHONPATH=/workspace/python
# or
PYTHONPATH=/workspace/python pytest tests/
```

### WebSocket Test Timeouts
Increase timeout for WebSocket tests:
```python
@pytest.mark.timeout(30)  # 30 second timeout
async def test_long_websocket_operation():
    # test code
```

## 🔄 Continuous Integration

Tests are automatically run on:
- Pull request creation
- Commits to main branch
- Nightly builds
- Manual trigger from UI

The CI pipeline includes:
1. Unit tests with coverage report
2. Integration tests
3. E2E tests (if applicable)
4. Performance benchmarks
5. Security scanning
