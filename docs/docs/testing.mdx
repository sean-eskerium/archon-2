---
title: Testing
sidebar_position: 8
---

# Testing Guide

Archon employs a comprehensive testing strategy covering unit tests, integration tests, end-to-end tests, and performance testing across all components of the system.

## üß™ Testing Philosophy

Our testing approach follows the testing pyramid:

```mermaid
%%{init:{
  'theme':'base',
  'themeVariables': {
    'primaryColor':'#1f2937',
    'primaryTextColor':'#ffffff',
    'primaryBorderColor':'#8b5cf6',
    'lineColor':'#a855f7',
    'textColor':'#ffffff',
    'fontFamily':'Inter',
    'fontSize':'14px',
    'background':'#000000',
    'mainBkg':'#1f2937',
    'secondBkg':'#111827',
    'borderColor':'#8b5cf6'
  }
}}%%
graph TB
    subgraph "Testing Pyramid"
        E2E["E2E Tests<br/>üîç User Journeys<br/>Selenium/Playwright"]
        Integration["Integration Tests<br/>üîó API & Database<br/>FastAPI TestClient"]
        Unit["Unit Tests<br/>‚ö° Functions & Classes<br/>pytest + mocks"]
    end

    Unit --> Integration
    Integration --> E2E
```

## üìÅ Test Directory Structure

```
python/
‚îú‚îÄ‚îÄ tests/                   # Backend test suite
‚îÇ   ‚îú‚îÄ‚îÄ unit/                # Unit tests
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_auth.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_vector_store.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_mcp_client.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_rag_pipeline.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_utils.py
‚îÇ   ‚îú‚îÄ‚îÄ integration/         # Integration tests
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_api_endpoints.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_database.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_mcp_server.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_websockets.py
‚îÇ   ‚îú‚îÄ‚îÄ e2e/                 # End-to-end tests
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_user_flows.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_document_upload.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_chat_interface.py
‚îÇ   ‚îú‚îÄ‚îÄ performance/         # Performance tests
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_load.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_stress.py
‚îÇ   ‚îú‚îÄ‚îÄ fixtures/            # Test data
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sample_documents/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mock_responses/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_data.json
‚îÇ   ‚îú‚îÄ‚îÄ conftest.py         # Pytest configuration
‚îÇ   ‚îî‚îÄ‚îÄ requirements-test.txt # Test dependencies
‚îÇ   ‚îî‚îÄ‚îÄ pytest.ini         # Pytest settings
‚îú‚îÄ‚îÄ src/                     # Application source code
‚îî‚îÄ‚îÄ pyproject.toml          # Python project configuration
```

## üöÄ Quick Start

### Prerequisites

```bash
# Navigate to python directory
cd python

# Install test dependencies
pip install -r requirements-test.txt

# Or install with development dependencies
pip install -e ".[dev]"
```

### Running Tests

```bash
# Run all tests (from python/ directory)
pytest

# Run with coverage
pytest --cov=src --cov-report=html

# Run specific test categories
pytest tests/unit/          # Unit tests only
pytest tests/integration/   # Integration tests only
pytest tests/e2e/          # E2E tests only

# Run tests with specific markers
pytest -m "not slow"       # Skip slow tests
pytest -m "api"            # Run API tests only
pytest -m "database"       # Run database tests only
```

### Test Configuration

```ini
# python/pytest.ini
[tool:pytest]
minversion = 6.0
addopts = 
    -ra
    --strict-markers
    --strict-config
    --cov=src
    --cov-branch
    --cov-report=term-missing:skip-covered
    --cov-report=html:htmlcov
    --cov-report=xml
    --cov-fail-under=80
testpaths = tests
markers =
    slow: marks tests as slow (deselect with '-m "not slow"')
    api: marks tests as API tests
    database: marks tests as database tests
    mcp: marks tests as MCP-related tests
    rag: marks tests as RAG-related tests
    auth: marks tests as authentication tests
    websocket: marks tests as WebSocket tests
```

## üîß Test Configuration & Fixtures

### conftest.py

```python
# python/tests/conftest.py
import pytest
import asyncio
from fastapi.testclient import TestClient
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from unittest.mock import Mock, patch

from src.main import app
from src.database import get_db, Base
from src.config import Settings
from src.auth import get_current_user

# Test database setup
SQLALCHEMY_DATABASE_URL = "sqlite:///./test.db"
engine = create_engine(SQLALCHEMY_DATABASE_URL, connect_args={"check_same_thread": False})
TestingSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

@pytest.fixture(scope="session")
def event_loop():
    """Create an instance of the default event loop for the test session."""
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()

@pytest.fixture(scope="function")
def db_session():
    """Create a fresh database session for each test."""
    Base.metadata.create_all(bind=engine)
    session = TestingSessionLocal()
    try:
        yield session
    finally:
        session.close()
        Base.metadata.drop_all(bind=engine)

@pytest.fixture(scope="function")
def client(db_session):
    """Create a test client with dependency overrides."""
    def override_get_db():
        try:
            yield db_session
        finally:
            db_session.close()

    def override_get_current_user():
        return {"id": "test-user", "email": "test@example.com"}

    app.dependency_overrides[get_db] = override_get_db
    app.dependency_overrides[get_current_user] = override_get_current_user

    with TestClient(app) as test_client:
        yield test_client

    app.dependency_overrides.clear()

@pytest.fixture
def mock_openai():
    """Mock OpenAI API calls."""
    with patch("openai.Embedding.create") as mock_embed, \
         patch("openai.ChatCompletion.create") as mock_chat:

        mock_embed.return_value = {
            "data": [{"embedding": [0.1] * 1536}]
        }

        mock_chat.return_value = {
            "choices": [{
                "message": {
                    "content": "Test response from OpenAI"
                }
            }]
        }

        yield {"embed": mock_embed, "chat": mock_chat}

@pytest.fixture
def mock_supabase():
    """Mock Supabase client."""
    mock_client = Mock()
    mock_client.table.return_value.insert.return_value.execute.return_value = {
        "data": [{"id": 1, "content": "test"}]
    }
    mock_client.table.return_value.select.return_value.execute.return_value = {
        "data": [{"id": 1, "content": "test", "embedding": [0.1] * 1536}]
    }
    return mock_client

@pytest.fixture
def sample_document():
    """Sample document for testing."""
    return {
        "title": "Test Document",
        "content": "This is a test document for RAG testing.",
        "metadata": {
            "source": "test",
            "type": "text"
        }
    }
```

## üß™ Unit Tests

### Authentication Tests

```python
# python/tests/unit/test_auth.py
import pytest
from unittest.mock import patch, Mock
from src.auth import create_access_token, verify_token, hash_password, verify_password

class TestAuthentication:

    def test_create_access_token(self):
        """Test JWT token creation."""
        data = {"sub": "test@example.com"}
        token = create_access_token(data)

        assert isinstance(token, str)
        assert len(token) > 0

    def test_verify_token_valid(self):
        """Test token verification with valid token."""
        data = {"sub": "test@example.com"}
        token = create_access_token(data)

        payload = verify_token(token)
        assert payload["sub"] == "test@example.com"

    def test_verify_token_invalid(self):
        """Test token verification with invalid token."""
        with pytest.raises(ValueError):
            verify_token("invalid.token.here")

    def test_password_hashing(self):
        """Test password hashing and verification."""
        password = "test_password_123"
        hashed = hash_password(password)

        assert hashed != password
        assert verify_password(password, hashed) is True
        assert verify_password("wrong_password", hashed) is False
```

### Vector Store Tests

```python
# python/tests/unit/test_vector_store.py
import pytest
import numpy as np
from unittest.mock import Mock, patch
from src.vector_store import VectorStore, generate_embedding, search_similar

class TestVectorStore:

    @pytest.fixture
    def vector_store(self, mock_supabase):
        """Create vector store instance with mocked dependencies."""
        return VectorStore(client=mock_supabase)

    def test_generate_embedding(self, mock_openai):
        """Test embedding generation."""
        text = "This is a test document"
        embedding = generate_embedding(text)

        assert isinstance(embedding, list)
        assert len(embedding) == 1536
        assert all(isinstance(x, float) for x in embedding)

    def test_search_similar(self, vector_store, mock_supabase):
        """Test similarity search."""
        query_embedding = [0.1] * 1536
        
        # Mock database response
        mock_supabase.rpc.return_value.execute.return_value = {
            "data": [
                {
                    "id": 1,
                    "content": "Similar document",
                    "similarity": 0.95
                }
            ]
        }

        results = vector_store.search_similar(query_embedding, limit=5)

        assert len(results) == 1
        assert results[0]["similarity"] >= 0.9
        mock_supabase.rpc.assert_called_once()
```

### RAG Pipeline Tests

```python
# python/tests/unit/test_rag_pipeline.py
import pytest
from unittest.mock import Mock, patch
from src.rag_pipeline import RAGPipeline, process_query, chunk_document

class TestRAGPipeline:

    @pytest.fixture
    def rag_pipeline(self, mock_supabase, mock_openai):
        """Create RAG pipeline with mocked dependencies."""
        return RAGPipeline(
            vector_store=Mock(),
            llm_client=Mock()
        )

    def test_chunk_document(self):
        """Test document chunking."""
        document = "This is a long document. " * 100
        chunks = chunk_document(document, chunk_size=200, overlap=50)

        assert len(chunks) > 1
        assert all(len(chunk) <= 250 for chunk in chunks)  # 200 + overlap
        assert len(chunks[0]) >= 200

    def test_process_query(self, rag_pipeline):
        """Test query processing."""
        query = "What is machine learning?"
        
        # Mock vector search results
        rag_pipeline.vector_store.search_similar.return_value = [
            {
                "content": "Machine learning is a subset of AI...",
                "similarity": 0.9
            }
        ]

        # Mock LLM response
        rag_pipeline.llm_client.chat.return_value = {
            "choices": [{
                "message": {
                    "content": "Machine learning is a method of data analysis..."
                }
            }]
        }

        response = rag_pipeline.process_query(query)

        assert "machine learning" in response.lower()
        rag_pipeline.vector_store.search_similar.assert_called_once()
```

## üîó Integration Tests

### API Endpoint Tests

```python
# python/tests/integration/test_api_endpoints.py
import pytest
import json
from fastapi.testclient import TestClient

class TestKnowledgeAPI:

    def test_upload_document(self, client):
        """Test document upload endpoint."""
        test_file = ("test.txt", "This is test content", "text/plain")
        
        response = client.post(
            "/api/documents/upload",
            files={"file": test_file},
            data={"knowledge_type": "technical"}
        )

        assert response.status_code == 200
        data = response.json()
        assert data["filename"] == "test.txt"
        assert "document_id" in data

    def test_search_knowledge(self, client, sample_document):
        """Test knowledge search endpoint."""
        # First upload a document
        test_file = ("sample.txt", sample_document["content"], "text/plain")
        upload_response = client.post(
            "/api/documents/upload",
            files={"file": test_file},
            data={"knowledge_type": "technical"}
        )
        assert upload_response.status_code == 200

        # Then search for it
        search_response = client.post(
            "/api/rag/query",
            json={
                "query": "test document",
                "limit": 5
            }
        )

        assert search_response.status_code == 200
        data = search_response.json()
        assert "results" in data
        assert len(data["results"]) > 0

    def test_list_documents(self, client):
        """Test document listing endpoint."""
        response = client.get("/api/knowledge-items")

        assert response.status_code == 200
        data = response.json()
        assert "items" in data
        assert "total" in data

    def test_delete_document(self, client):
        """Test document deletion."""
        # Upload a document first
        test_file = ("delete_test.txt", "Content to delete", "text/plain")
        upload_response = client.post(
            "/api/documents/upload",
            files={"file": test_file}
        )
        
        document_id = upload_response.json()["document_id"]

        # Delete the document
        delete_response = client.delete(f"/api/knowledge-items/{document_id}")
        
        assert delete_response.status_code == 200
        assert "deleted" in delete_response.json()["message"].lower()
```

### Database Tests

```python
# python/tests/integration/test_database.py
import pytest
from src.database import get_supabase_client, create_tables, insert_document

class TestDatabase:

    def test_supabase_connection(self):
        """Test Supabase connection."""
        client = get_supabase_client()
        
        # Test basic operation
        response = client.table("crawled_pages").select("*").limit(1).execute()
        assert "data" in response

    def test_insert_document(self, sample_document):
        """Test document insertion."""
        client = get_supabase_client()
        
        result = insert_document(
            client=client,
            content=sample_document["content"],
            metadata=sample_document["metadata"],
            embedding=[0.1] * 1536
        )

        assert result["success"] is True
        assert "id" in result

    def test_vector_search(self):
        """Test vector similarity search."""
        client = get_supabase_client()
        
        query_embedding = [0.1] * 1536
        
        response = client.rpc(
            "match_crawled_pages",
            {
                "query_embedding": query_embedding,
                "match_count": 5,
                "filter": {}
            }
        ).execute()

        assert "data" in response
        assert isinstance(response["data"], list)
```

### MCP Server Tests

```python
# python/tests/integration/test_mcp_server.py
import pytest
import asyncio
import json
from unittest.mock import Mock
from src.mcp_server import MCPServer

class TestMCPServer:

    @pytest.fixture
    async def mcp_server(self):
        """Create MCP server instance."""
        server = MCPServer()
        await server.initialize()
        return server

    @pytest.mark.asyncio
    async def test_search_tool(self, mcp_server):
        """Test MCP search tool."""
        request = {
            "tool": "search_knowledge",
            "arguments": {
                "query": "test query",
                "limit": 5
            }
        }

        response = await mcp_server.call_tool(request)

        assert response["success"] is True
        assert "results" in response

    @pytest.mark.asyncio
    async def test_create_task_tool(self, mcp_server):
        """Test MCP task creation tool."""
        request = {
            "tool": "create_task",
            "arguments": {
                "title": "Test Task",
                "description": "This is a test task",
                "project_id": "test-project"
            }
        }

        response = await mcp_server.call_tool(request)

        assert response["success"] is True
        assert response["task"]["title"] == "Test Task"
```

## üåê End-to-End Tests

### User Flow Tests

```python
# python/tests/e2e/test_user_flows.py
import pytest
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

class TestUserFlows:

    @pytest.fixture(scope="class")
    def driver(self):
        """Create web driver instance."""
        options = webdriver.ChromeOptions()
        options.add_argument("--headless")
        driver = webdriver.Chrome(options=options)
        driver.implicitly_wait(10)
        yield driver
        driver.quit()

    def test_complete_document_workflow(self, driver):
        """Test complete document upload and search workflow."""
        # Navigate to application
        driver.get("http://localhost:3737")

        # Upload document
        upload_button = driver.find_element(By.TEXT, "Upload Document")
        upload_button.click()

        file_input = driver.find_element(By.CSS_SELECTOR, "input[type='file']")
        file_input.send_keys("/path/to/test/document.pdf")

        submit_button = driver.find_element(By.TEXT, "Upload")
        submit_button.click()

        # Wait for upload completion
        WebDriverWait(driver, 30).until(
            EC.presence_of_element_located((By.TEXT, "Upload successful"))
        )

        # Search for uploaded document
        search_input = driver.find_element(By.CSS_SELECTOR, "input[placeholder*='Search']")
        search_input.send_keys("test document content")
        search_input.submit()

        # Verify search results
        results = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CLASS_NAME, "search-results"))
        )

        assert len(results.find_elements(By.CLASS_NAME, "result-item")) > 0

    def test_mcp_connection_workflow(self, driver):
        """Test MCP connection setup workflow."""
        driver.get("http://localhost:3737")

        # Navigate to MCP settings
        settings_link = driver.find_element(By.TEXT, "Settings")
        settings_link.click()

        mcp_tab = driver.find_element(By.TEXT, "MCP Server")
        mcp_tab.click()

        # Start MCP server
        start_button = driver.find_element(By.TEXT, "Start MCP Server")
        start_button.click()

        # Wait for server to start
        status_indicator = WebDriverWait(driver, 15).until(
            EC.text_to_be_present_in_element(
                (By.CLASS_NAME, "server-status"), "Running"
            )
        )

        # Verify connection info is displayed
        connection_info = driver.find_element(By.CLASS_NAME, "connection-info")
        assert "stdio" in connection_info.text.lower()
```

## ‚ö° Performance Tests

### Load Testing

```python
# python/tests/performance/test_load.py
import pytest
import asyncio
import aiohttp
import time
from concurrent.futures import ThreadPoolExecutor

class TestLoadPerformance:

    @pytest.mark.asyncio
    async def test_api_concurrent_requests(self):
        """Test API performance under concurrent load."""
        async def make_request(session, url):
            async with session.get(url) as response:
                return await response.json()

        async with aiohttp.ClientSession() as session:
            tasks = []
            for _ in range(50):  # 50 concurrent requests
                task = make_request(session, "http://localhost:8080/api/knowledge-items")
                tasks.append(task)

            start_time = time.time()
            responses = await asyncio.gather(*tasks)
            end_time = time.time()

            # All requests should complete within 10 seconds
            assert end_time - start_time < 10
            # All responses should be successful
            assert all("items" in response for response in responses)

    def test_document_upload_performance(self):
        """Test document upload performance."""
        import requests

        # Create a test document
        test_content = "This is test content. " * 1000  # ~20KB
        files = {"file": ("large_test.txt", test_content, "text/plain")}

        start_time = time.time()
        response = requests.post("http://localhost:8080/api/documents/upload", files=files)
        end_time = time.time()

        upload_time = end_time - start_time

        assert response.status_code == 200
        assert upload_time < 10.0  # Upload should complete within 10 seconds

    @pytest.mark.asyncio
    async def test_search_performance(self):
        """Test search performance with multiple queries."""
        queries = [
            "machine learning",
            "artificial intelligence", 
            "data science",
            "python programming",
            "web development"
        ]

        async def search_query(session, query):
            payload = {"query": query, "limit": 10}
            async with session.post(
                "http://localhost:8080/api/rag/query",
                json=payload
            ) as response:
                return await response.json()

        async with aiohttp.ClientSession() as session:
            start_time = time.time()
            
            tasks = [search_query(session, query) for query in queries]
            results = await asyncio.gather(*tasks)
            
            end_time = time.time()

            # All searches should complete within 5 seconds
            assert end_time - start_time < 5.0
            # All searches should return results
            assert all("results" in result for result in results)
```

## üîç Test Coverage & Reporting

### Coverage Configuration

```ini
# python/.coveragerc
[run]
source = src
omit = 
    python/src/migrations/*
    python/tests/*
    python/src/config.py
    */venv/*
    */virtualenv/*

[report]
exclude_lines =
    pragma: no cover
    def __repr__
    raise AssertionError
    raise NotImplementedError
    if __name__ == .__main__.:
    class .*Protocol\):
    @(abc\.)?abstractmethod

[html]
directory = htmlcov
```

### Generating Reports

```bash
# Navigate to python directory first
cd python

# Generate coverage report
pytest --cov=src --cov-report=html --cov-report=term

# Generate XML report for CI/CD
pytest --cov=src --cov-report=xml

# Generate detailed coverage report
coverage report --show-missing
```

## üöÄ Continuous Integration

### GitHub Actions Workflow

```yaml
# .github/workflows/test.yml
name: Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.9, 3.10, 3.11]

    services:
      postgres:
        image: postgres:13
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v3
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install dependencies
      run: |
        cd python
        python -m pip install --upgrade pip
        pip install -r requirements-test.txt

    - name: Run unit tests
      run: |
        cd python
        pytest tests/unit/ --cov=src --cov-report=xml

    - name: Run integration tests
      run: |
        cd python
        pytest tests/integration/ --cov=src --cov-append --cov-report=xml
      env:
        DATABASE_URL: postgresql://postgres:postgres@localhost/test_db

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./python/coverage.xml
        flags: unittests
        name: codecov-umbrella
```

## üõ†Ô∏è Testing Best Practices

### Test Organization

1. **Follow AAA Pattern**: Arrange, Act, Assert
2. **Use descriptive test names**: `test_should_return_error_when_invalid_token_provided`
3. **One assertion per test**: Focus on testing one behavior
4. **Use fixtures for setup**: Avoid code duplication
5. **Mock external dependencies**: Keep tests isolated

### Test Data Management

```python
# Use factories for test data
class DocumentFactory:
    @staticmethod
    def create_sample_document(**kwargs):
        defaults = {
            "title": "Sample Document",
            "content": "This is sample content for testing.",
            "metadata": {"source": "test", "type": "text"}
        }
        defaults.update(kwargs)
        return defaults

# Use parametrized tests for multiple scenarios
@pytest.mark.parametrize("input_text,expected_length", [
    ("short", 5),
    ("medium length text", 18),
    ("this is a much longer text for testing", 38)
])
def test_text_processing(input_text, expected_length):
    result = process_text(input_text)
    assert len(result) == expected_length
```

### Performance Testing Guidelines

1. **Set realistic thresholds**: Based on production requirements
2. **Test under load**: Simulate concurrent users
3. **Monitor resource usage**: CPU, memory, database connections
4. **Test edge cases**: Large files, long conversations
5. **Automate performance tests**: Include in CI/CD pipeline

## üêõ Debugging Tests

### Common Issues

```bash
# Navigate to python directory first
cd python

# Run tests with verbose output
pytest -v

# Run specific test with debugging
pytest tests/unit/test_auth.py::test_create_access_token -v -s

# Drop into debugger on failure
pytest --pdb

# Show local variables on failure
pytest --tb=long
```

### Test Environment Setup

```bash
# Create isolated test environment
python -m venv test_env
source test_env/bin/activate
cd python
pip install -r requirements-test.txt

# Run tests with environment variables
TEST_DATABASE_URL=sqlite:///test.db pytest
```

## üîß Docker Testing

### Running Tests in Docker

```bash
# Build and run tests in Docker
docker-compose run --rm archon-pyserver pytest

# Run specific test suite
docker-compose run --rm archon-pyserver pytest tests/unit/

# Run with coverage
docker-compose run --rm archon-pyserver pytest --cov=src --cov-report=html
```

### Test Database Setup

```yaml
# docker-compose.test.yml
version: '3.8'

services:
  test-db:
    image: postgres:13
    environment:
      POSTGRES_DB: test_archon
      POSTGRES_USER: test_user
      POSTGRES_PASSWORD: test_password
    ports:
      - "5433:5432"

  archon-pyserver-test:
    build: ./python
    depends_on:
      - test-db
    environment:
      - DATABASE_URL=postgresql://test_user:test_password@test-db:5432/test_archon
      - TESTING=true
    command: pytest --cov=src --cov-report=xml
    volumes:
      - ./python/tests:/app/tests
      - ./python/src:/app/src
```

---

**Next Steps:**
- Learn about [Deployment](./deployment) for production setup
- Explore [API Reference](./api-reference) for endpoint testing
- Check [Server Architecture](./server) for system understanding
