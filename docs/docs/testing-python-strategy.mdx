---
title: Python Testing Strategy
sidebar_position: 9
---

# Python Testing Strategy with pytest

This document outlines Archon's comprehensive Python testing strategy, incorporating the latest pytest best practices for 2024. Our test suite includes **235+ test cases** across unit, integration, E2E, and performance tests.

## 🎯 Testing Philosophy

Our Python testing follows these core principles:

1. **Test Isolation**: Each test should be independent and self-contained
2. **Descriptive Naming**: Long, descriptive test names that explain what is being tested
3. **Behavior-Driven**: Test behavior and outcomes, not implementation details
4. **Comprehensive Coverage**: Aim for 80%+ code coverage (95% for critical services)
5. **Fast Feedback**: Tests should run quickly to enable rapid development

## 📁 Project Structure

```
python/
├── pyproject.toml           # Modern Python project configuration
├── tests/                   # Test suite (235+ test cases)
│   ├── conftest.py         # Global fixtures and configuration
│   ├── pytest.ini          # Pytest configuration
│   ├── .coveragerc         # Coverage settings
│   ├── run_tests.py        # Custom test runner with filtering
│   ├── unit/               # Unit tests
│   │   ├── test_agents/    # Agent tests (30 cases)
│   │   ├── test_api/       # API endpoint tests (65 cases)
│   │   ├── test_models/    # Model validation tests
│   │   ├── test_modules/   # Module tests
│   │   ├── test_services/  # Service layer tests (137 cases)
│   │   │   ├── test_projects/   # ProjectService, TaskService, etc.
│   │   │   └── test_rag/        # RAG service tests
│   │   └── test_utils/     # Utility tests (12 cases)
│   ├── integration/        # Integration tests
│   │   ├── test_api/       # API integration tests
│   │   ├── test_database/  # Database integration
│   │   ├── test_mcp/       # MCP integration
│   │   └── test_websockets/# WebSocket tests
│   ├── e2e/                # End-to-end tests
│   │   ├── test_workflows/ # Complete user workflows
│   │   └── test_mcp_tools/ # MCP tool integration
│   ├── performance/        # Performance tests
│   └── fixtures/           # Test data and utilities
│       ├── mock_data.py    # Factory patterns & builders
│       └── test_helpers.py # Domain-specific assertions
├── src/                    # Application source code
```

## 🔧 Configuration

### pytest.ini

```ini
[pytest]
# Minimum pytest version required
minversion = 8.0

# Default command line options
addopts = 
    -ra
    --strict-markers
    --import-mode=importlib
    --tb=short
    --cov=src
    --cov-report=term-missing:skip-covered
    --cov-report=html
    --cov-report=xml
    --cov-fail-under=80

# Test discovery paths
testpaths = tests

# Test discovery patterns
python_files = test_*.py
python_classes = Test*
python_functions = test_*

# Required plugins
required_plugins = 
    pytest-asyncio>=0.21.0
    pytest-timeout>=2.1.0
    pytest-mock>=3.10.0
    pytest-cov>=4.0.0

# Test markers
markers =
    # Priority markers
    critical: Critical priority tests that must pass
    high: High priority tests
    standard: Standard priority tests
    nice_to_have: Nice-to-have tests
    
    # Test type markers
    unit: Fast, isolated unit tests
    integration: Integration tests requiring external resources
    e2e: End-to-end workflow tests
    performance: Performance benchmark tests
    
    # Speed markers
    slow: Tests that take > 1s to run
    
    # Feature markers
    websocket: Tests involving WebSocket functionality
    sse: Tests involving Server-Sent Events
    mcp: Tests for MCP functionality
    rag: Tests for RAG functionality

# Async configuration
asyncio_mode = auto
```

## 🧪 Testing Patterns

### 1. Service Layer Testing with Enhanced Patterns

```python
# tests/unit/test_services/test_projects/test_project_service.py
import pytest
from unittest.mock import Mock, AsyncMock, MagicMock
from tests.fixtures.mock_data import ProjectFactory, ProjectBuilder
from tests.fixtures.test_helpers import (
    assert_valid_project_state,
    assert_api_response_valid,
    assert_called_with_subset,
    PerformanceTimer
)

@pytest.mark.unit
@pytest.mark.critical
class TestProjectService:
    """Unit tests for ProjectService with enhanced patterns."""
    
    @pytest.fixture
    def mock_supabase_client(self):
        """Mock Supabase client with chained method support."""
        mock = MagicMock()
        # Setup chainable methods
        for method in ["table", "select", "insert", "eq", "execute"]:
            setattr(mock, method, MagicMock(return_value=mock))
        return mock
    
    @pytest.fixture
    def project_service(self, mock_supabase_client):
        """Create ProjectService instance with mocked dependencies."""
        return ProjectService(supabase_client=mock_supabase_client)
    
    @pytest.mark.parametrize("title,prd,github_repo,expected_success", [
        pytest.param("Valid Project", {"description": "Test"}, "org/repo", True, id="valid-all-fields"),
        pytest.param("Minimal Project", None, None, True, id="valid-minimal"),
        pytest.param("", None, None, False, id="invalid-empty-title"),
        pytest.param(None, None, None, False, id="invalid-none-title"),
    ])
    def test_create_project_validation(
        self, project_service, mock_supabase_client, title, prd, github_repo, expected_success
    ):
        """Test project creation with various input combinations."""
        # Arrange
        if expected_success:
            mock_project = ProjectFactory.create(title=title)
            mock_supabase_client.execute.return_value = MagicMock(data=[mock_project])
        
        # Act
        success, result = project_service.create_project(
            title=title, prd=prd, github_repo=github_repo
        )
        
        # Assert
        assert success == expected_success
        
        if expected_success:
            assert_valid_project_state(result["project"])
            assert_called_with_subset(mock_supabase_client.insert, title=title)
        else:
            assert_api_response_valid(result, success=False)
            assert "title is required" in result["error"]
    
    @pytest.mark.slow
    @pytest.mark.performance
    def test_list_projects_performance_with_large_datasets(
        self, project_service, mock_supabase_client
    ):
        """Test list_projects performance with large datasets."""
        # Arrange - Use builder pattern for complex test data
        test_scenario = (ProjectBuilder()
            .with_title("Performance Test Project")
            .with_tasks(count=100)
            .with_documents(count=50)
            .build())
        
        mock_supabase_client.execute.return_value = MagicMock(
            data=[test_scenario['project']]
        )
        
        # Act & Assert
        with PerformanceTimer("list_projects", threshold=0.1):
            success, result = project_service.list_projects()
        
        assert success is True
```

### 2. Async Testing with Proper Patterns

```python
# tests/unit/test_services/test_projects/test_task_service.py
import pytest
from tests.fixtures.test_helpers import assert_async_timeout, assert_websocket_receives

@pytest.mark.unit
@pytest.mark.critical
class TestTaskService:
    """Unit tests for TaskService with async patterns."""
    
    @pytest.mark.asyncio
    @pytest.mark.timeout(5)
    async def test_create_task_with_websocket_notification(
        self, task_service, mock_websocket_manager
    ):
        """Test task creation broadcasts WebSocket notification."""
        # Arrange
        task_data = TaskFactory.create()
        mock_websocket_manager.broadcast = AsyncMock()
        
        # Act
        success, result = await task_service.create_task(
            project_id="proj_123",
            title="Test Task"
        )
        
        # Assert
        assert success is True
        
        # Verify WebSocket broadcast with timeout
        await assert_async_timeout(
            mock_websocket_manager.broadcast.assert_called_once(),
            timeout=1.0,
            error_msg="WebSocket broadcast not called"
        )
```

### 3. Test Data Builders

```python
# tests/fixtures/mock_data.py
class ProjectBuilder:
    """Builder pattern for creating complex project test data."""
    
    def __init__(self):
        self._project = ProjectFactory.create()
        self._tasks = []
        self._documents = []
    
    def with_title(self, title: str) -> 'ProjectBuilder':
        self._project['title'] = title
        return self
    
    def with_task_hierarchy(self, depth: int = 2, width: int = 3) -> 'ProjectBuilder':
        """Add hierarchical tasks to the project."""
        # Implementation creates nested tasks
        return self
    
    def build(self) -> Dict[str, Any]:
        return {
            'project': self._project,
            'tasks': self._tasks,
            'documents': self._documents
        }

# Usage in tests
test_project = (ProjectBuilder()
    .with_title("Complex Project")
    .with_task_hierarchy(depth=3)
    .with_documents(count=5)
    .build())
```

### 4. Domain-Specific Assertions

```python
# tests/fixtures/test_helpers.py
def assert_valid_project_state(project: Dict[str, Any]) -> None:
    """Assert project is in a valid state."""
    assert_valid_uuid(project['id'], "Project ID")
    assert project.get('title', '').strip(), "Project title cannot be empty"
    assert project.get('status') in ['active', 'archived', 'draft']
    assert project['created_at'] <= project['updated_at']

def assert_task_hierarchy_valid(tasks: List[Dict[str, Any]]) -> None:
    """Assert task parent-child relationships are valid."""
    task_ids = {t['id'] for t in tasks}
    for task in tasks:
        if task.get('parent_task_id'):
            assert task['parent_task_id'] in task_ids
```

## 🧪 Test Categories by Priority

### Critical Priority Tests (137 cases)
- **ProjectService** (15 cases): CRUD, validation, linked sources
- **TaskService** (14 cases): Status transitions, subtasks, archiving, WebSocket
- **DocumentService** (12 cases): JSONB operations, version snapshots
- **CredentialService** (13 cases): Encryption/decryption, environment variables
- **MCPClientService** (12 cases): SSE transport, tool execution, auto-reconnection
- **VersioningService** (10 cases): Version history, restoration
- **MCPSessionManager** (12 cases): Session lifecycle, expiration
- **CrawlingService** (11 cases): Single/batch crawling, robots.txt
- **DocumentStorageService** (13 cases): Chunking, embeddings, batch operations
- **SearchService** (13 cases): Vector search, reranking, hybrid search
- **Utils** (12 cases): Embeddings, Supabase client, text extraction

### High Priority Tests (20 cases)
- **PromptService** (10 cases): Singleton pattern, template caching
- **SourceManagementService** (10 cases): Source CRUD, metadata management

### Standard Priority Tests (78+ cases)
- **BaseAgent** (10 cases): Rate limiting, streaming, timeout handling
- **DocumentAgent** (8 cases): Conversational document interactions
- **RagAgent** (12 cases): Search and retrieval operations
- **API Endpoints**: Projects, Settings, Knowledge, MCP, Agent Chat

## 🚀 Testing Commands

### Using the Custom Test Runner

```bash
# Run all tests with the custom runner
python tests/run_tests.py

# Run by test type
python tests/run_tests.py --type unit
python tests/run_tests.py --type integration
python tests/run_tests.py --type e2e

# Run by priority
python tests/run_tests.py --priority critical
python tests/run_tests.py --priority high

# Combine filters
python tests/run_tests.py --type unit --priority critical

# Run with coverage
python tests/run_tests.py --coverage
```

### Direct pytest Commands

```bash
# Run all tests
PYTHONPATH=/workspace/python pytest

# Run with specific markers
pytest -m "critical and not slow"
pytest -m "websocket or sse"

# Run specific test files
pytest tests/unit/test_services/test_projects/

# Run with different verbosity
pytest -v     # Verbose
pytest -vv    # Very verbose
pytest -q     # Quiet

# Debugging
pytest --pdb  # Drop into debugger on failure
pytest -l     # Show local variables
pytest --lf   # Run last failed
pytest --ff   # Run failed first
```

## 📊 Code Coverage

### Coverage Goals
- **Overall**: 80% minimum
- **Critical Services**: 95% minimum
- **API Endpoints**: 90% minimum
- **New Code**: 90%+ required

### Coverage Reports

```bash
# Generate HTML report
pytest --cov=src --cov-report=html
# Open htmlcov/index.html

# Terminal report with missing lines
pytest --cov=src --cov-report=term-missing

# XML for CI/CD
pytest --cov=src --cov-report=xml

# Enforce coverage threshold
pytest --cov=src --cov-fail-under=80
```

## 🔄 WebSocket Test Integration

The test suite integrates with Archon's UI through WebSocket connections:

```python
# src/api/tests_api.py
async def execute_mcp_tests(execution_id: str) -> TestExecution:
    """Execute Python tests with real-time streaming."""
    cmd = [
        "python", "tests/run_tests.py",
        "--type", "unit",
        "--no-capture",
        "--verbose"
    ]
    
    # Stream results to UI via WebSocket
    async for line in process.stdout:
        await websocket_manager.broadcast_test_update(
            execution_id=execution_id,
            output=line.decode()
        )
```

## 🎯 Best Practices We Follow

1. **Single Responsibility**: Each test verifies ONE behavior
2. **Descriptive Names**: `test_<method>_<scenario>_<expected_result>`
3. **AAA Pattern**: Arrange, Act, Assert structure
4. **Proper Mocking**: Always use `autospec=True`
5. **Parametrization**: Use `@pytest.mark.parametrize` for test variations
6. **Async Timeouts**: Explicit timeouts for all async tests
7. **Performance Tests**: Monitor for regressions
8. **Test Isolation**: No dependencies between tests

## 📚 Additional Resources

- [Test Style Guide](../TEST_STYLE_GUIDE.md)
- [Pytest Best Practices Review](../PYTEST_BEST_PRACTICES_REVIEW.md)
- [Testing Overview](./testing)
- [Official pytest Documentation](https://docs.pytest.org/)